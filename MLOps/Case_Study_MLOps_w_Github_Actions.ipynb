{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["LPPvT9ymC3IT","qGihLYlPigGF","LN0KWTL4i2B-","flzauIsdrh7p","0LbSu_p2jYfe","9DtS3gNDjBbR","hh2TjRG5WJ4Z","eZZKnLkLjeM4","07cYzWcIwTL-","V4ynzpKNwWS_","PuCgAW2hktli","yo_xWLLvkqSC","PvEUJ-t5kdxH","BA6mP-Ebkm3O"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Problem Statement"],"metadata":{"id":"LPPvT9ymC3IT"}},{"cell_type":"markdown","source":["## Business Context"],"metadata":{"id":"qGihLYlPigGF"}},{"cell_type":"markdown","source":["In the competitive landscape of retail banking, customer retention is critical for ensuring sustainable growth and profitability. A prominent retail banking institution in Europe provides a range of financial products, including credit cards, loans, and savings accounts, and has been rapidly expanding its customer base across multiple countries. However, with a growing customer base, it faces an increasingly pressing challenge: customer churn. A significant number of customers are closing their accounts and switching to competitors. This decline in customer retention is impacting revenue and long-term customer relationships\n","\n","Understanding the reasons behind customer attrition (or churn) is essential for the bank to devise effective retention strategies to minimize churn and enhance customer loyalty and satisfaction. The Customer Analytics & Retention Department has been diligently collecting and analyzing historical customer data. Despite the valuable insights provided by historical data, the department grapples with several challenges:\n","\n","1. **Complex Customer Behavior**: The diverse nature of the bank's offerings and the varying customer preferences across different countries complicate the identification of factors that lead to churn.\n","2. **Proactive Retention**: The current processes for identifying at-risk customers are reactive rather than proactive, leading to missed opportunities for timely interventions that could prevent churn."],"metadata":{"id":"pcYVQzSbllJN"}},{"cell_type":"markdown","source":["## Objective"],"metadata":{"id":"LN0KWTL4i2B-"}},{"cell_type":"markdown","source":["To overcome the limitations of traditional machine learning workflows—such as manual execution of data preparation, model training, testing, versioning, and deployment—the organization has hired you as a data scientist to implement a robust MLOps pipeline using GitHub Actions on Hugging Face. The objective is to build an automated and reproducible MLOps pipeline that streamlines the entire ML lifecycle—from code integration to model deployment—ensuring faster, more reliable access to the churn prediction model for geographically distributed teams, and enabling proactive, data-driven customer retention strategies.\n"],"metadata":{"id":"XupSlU6QlmDQ"}},{"cell_type":"markdown","source":["## Pre-requisites"],"metadata":{"id":"flzauIsdrh7p"}},{"cell_type":"markdown","source":["* Create a Github repo\n","    - Go to ***Github Profile***\n","    - Click on ***Your repositories*** then select ***New***\n","      - Repository Name: ***MLOps***\n","      - Check the box ***README.md*** file\n","      - Click on ***Create repository***\n","\n","* Adding hugging face space secrets to Github Actions to execute the workflow\n","  1. Go to Hugging Face ***Profile***\n","  2. Navigate to ***Access Token***\n","  3. Create a ***New token***\n","      - Token type ***Write***\n","      - Token Name ***MLOps***\n","      - Click on ***Create Token***\n","      - Copy the generated Token\n","  4. Now, go to Github repo\n","      - Click on ***Settings***\n","      - Navigate to ***Secrets and Variables***\n","      - Click on ***Actions***\n","      - Add a ***Repository secerts***\n","        - Name ***HF_TOKEN***\n","        - Secret: ***Paste the token created from the hugging face access tokens***\n","        - Click on ***Add secret***\n","\n","* Create a Hugging Face space\n","    - Go to **Hugging Face**\n","    - Open your **Profile**\n","    - Click on **New Space**\n","      - Under the space creation, enter the below details\n","        - Space name: **Bank-Customer-Churn**\n","    (If you were trying with different names, be cautious when using a underscore `_` in space names, such as `frontend_space`, as it can cause exceptions when accessing the API URL. Always use an hyphen `-` instead, like `frontend-space`.)\n","        - Select the space SDK: **Docker**\n","        - Choose a Docker template: **Streamlit**\n","        - Click on **Create Space**"],"metadata":{"id":"RQd1G680rfpE"}},{"cell_type":"code","source":["!pip install numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 joblib==1.4.2 xgboost==2.1.4 requests==2.32.3 huggingface_hub==0.30.1 -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENJ1aZLSdNee","executionInfo":{"status":"ok","timestamp":1769584540387,"user_tz":-330,"elapsed":21130,"user":{"displayName":"Manju krishna","userId":"17824596203469046751"}},"outputId":"4d02be29-d50e-4d06-cfba-83cd145014d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n","transformers 4.57.6 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.30.1 which is incompatible.\n","diffusers 0.36.0 requires huggingface-hub<2.0,>=0.34.0, but you have huggingface-hub 0.30.1 which is incompatible.\n","gradio 5.50.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.30.1 which is incompatible.\n","google-adk 1.21.0 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Libraries to help with reading and manipulating data\n","import numpy as np\n","import pandas as pd\n","\n","# For splitting the dataset\n","from sklearn.model_selection import train_test_split\n","\n","# Libaries to help with data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Removes the limit for the number of displayed columns\n","pd.set_option(\"display.max_columns\", None)\n","# Sets the limit for the number of displayed rows\n","pd.set_option(\"display.max_rows\", 100)\n","\n","\n","# Libraries different ensemble classifiers\n","from sklearn.ensemble import (\n","    BaggingRegressor,\n","    RandomForestRegressor,\n","    AdaBoostRegressor,\n","    GradientBoostingRegressor,\n",")\n","from xgboost import XGBRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Libraries to get different metric scores\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    mean_squared_error,\n","    mean_absolute_error,\n","    r2_score,\n","    mean_absolute_percentage_error\n",")\n","\n","# To create the pipeline\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline,Pipeline\n","\n","# To tune different models and standardize\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler,OneHotEncoder\n","from sklearn import metrics\n","\n","# To serialize the model\n","import joblib\n","\n","# os related functionalities\n","import os\n","\n","# API request\n","import requests\n","\n","# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi"],"metadata":{"id":"p7UXBC41dNhD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4z5v2MWycXB9","executionInfo":{"status":"ok","timestamp":1769586202957,"user_tz":-330,"elapsed":1522,"user":{"displayName":"Manju krishna","userId":"17824596203469046751"}},"outputId":"31722928-5b77-4a70-aaf7-e47b852d1585"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Create a master folder to keep all files created when executing the below code cells\n","# import os\n","# os.makedirs(\"MLops\", exist_ok=True)\n","\n","import os\n","os.makedirs(\"/content/drive/My Drive/Colab Notebooks/MLOps/data\", exist_ok=True)"],"metadata":{"id":"giodc4KknHID","executionInfo":{"status":"ok","timestamp":1769586472202,"user_tz":-330,"elapsed":10,"user":{"displayName":"Manju krishna","userId":"17824596203469046751"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Model Building"],"metadata":{"id":"0LbSu_p2jYfe"}},{"cell_type":"markdown","source":["## Data Registration"],"metadata":{"id":"9DtS3gNDjBbR"}},{"cell_type":"code","source":["os.makedirs(\"MLops/data\", exist_ok=True)"],"metadata":{"id":"kNUYcTe-xckI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the **data** folder created after executing the above cell, please upload the **bank_customer_churn.csv** in to the folder"],"metadata":{"id":"WxXiD9ZXxodF"}},{"cell_type":"code","source":["# Create a folder for storing the model building files\n","os.makedirs(\"MLops/model_building\", exist_ok=True)"],"metadata":{"id":"SUKPoy0EA4jj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLXRfi2VAgNR"},"outputs":[],"source":["%%writefile mlops/model_building/data_register.py\n","from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n","from huggingface_hub import HfApi, create_repo\n","import os\n","\n","\n","repo_id = \"<----------Hugging Face User ID------->/bank-customer-churn\"\n","repo_type = \"dataset\"\n","\n","# Initialize API client\n","api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n","\n","# Step 1: Check if the space exists\n","try:\n","    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n","    print(f\"Space '{repo_id}' already exists. Using it.\")\n","except RepositoryNotFoundError:\n","    print(f\"Space '{repo_id}' not found. Creating new space...\")\n","    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n","    print(f\"Space '{repo_id}' created.\")\n","\n","api.upload_folder(\n","    folder_path=\"mlops/data\",\n","    repo_id=repo_id,\n","    repo_type=repo_type,\n",")"]},{"cell_type":"markdown","source":["## Data Preparation"],"metadata":{"id":"hh2TjRG5WJ4Z"}},{"cell_type":"code","source":["%%writefile mlops/model_building/prep.py\n","# for data manipulation\n","import pandas as pd\n","import sklearn\n","# for creating a folder\n","import os\n","# for data preprocessing and pipeline creation\n","from sklearn.model_selection import train_test_split\n","# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi\n","\n","# Define constants for the dataset and output paths\n","api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n","DATASET_PATH = \"hf://datasets/<---------Huggnig Face User ID--------->/bank-customer-churn/bank_customer_churn.csv\"\n","bank_dataset = pd.read_csv(DATASET_PATH)\n","print(\"Dataset loaded successfully.\")\n","\n","# Define the target variable for the classification task\n","target = 'Exited'\n","\n","# List of numerical features in the dataset\n","numeric_features = [\n","    'CreditScore',       # Customer's credit score\n","    'Age',               # Customer's age\n","    'Tenure',            # Number of years the customer has been with the bank\n","    'Balance',           # Customer’s account balance\n","    'NumOfProducts',     # Number of products the customer has with the bank\n","    'HasCrCard',         # Whether the customer has a credit card (binary: 0 or 1)\n","    'IsActiveMember',    # Whether the customer is an active member (binary: 0 or 1)\n","    'EstimatedSalary'    # Customer’s estimated salary\n","]\n","\n","# List of categorical features in the dataset\n","categorical_features = [\n","    'Geography',         # Country where the customer resides\n","]\n","\n","# Define predictor matrix (X) using selected numeric and categorical features\n","X = bank_dataset[numeric_features + categorical_features]\n","\n","# Define target variable\n","y = bank_dataset[target]\n","\n","\n","# Split dataset into train and test\n","# Split the dataset into training and test sets\n","Xtrain, Xtest, ytrain, ytest = train_test_split(\n","    X, y,              # Predictors (X) and target variable (y)\n","    test_size=0.2,     # 20% of the data is reserved for testing\n","    random_state=42    # Ensures reproducibility by setting a fixed random seed\n",")\n","\n","Xtrain.to_csv(\"Xtrain.csv\",index=False)\n","Xtest.to_csv(\"Xtest.csv\",index=False)\n","ytrain.to_csv(\"ytrain.csv\",index=False)\n","ytest.to_csv(\"ytest.csv\",index=False)\n","\n","\n","files = [\"Xtrain.csv\",\"Xtest.csv\",\"ytrain.csv\",\"ytest.csv\"]\n","\n","for file_path in files:\n","    api.upload_file(\n","        path_or_fileobj=file_path,\n","        path_in_repo=file_path.split(\"/\")[-1],  # just the filename\n","        repo_id=\"<-------Hugging Face User ID-------->/bank-customer-churn\",\n","        repo_type=\"dataset\",\n","    )"],"metadata":{"id":"whml0T_rBNGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"eZZKnLkLjeM4"}},{"cell_type":"code","source":["%%writefile mlops/model_building/train.py\n","# for data manipulation\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline\n","# for model training, tuning, and evaluation\n","import xgboost as xgb\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, recall_score\n","# for model serialization\n","import joblib\n","# for creating a folder\n","import os\n","# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi, create_repo\n","from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n","\n","api = HfApi()\n","\n","Xtrain_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/Xtrain.csv\"\n","Xtest_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/Xtest.csv\"\n","ytrain_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/ytrain.csv\"\n","ytest_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/ytest.csv\"\n","\n","Xtrain = pd.read_csv(Xtrain_path)\n","Xtest = pd.read_csv(Xtest_path)\n","ytrain = pd.read_csv(ytrain_path)\n","ytest = pd.read_csv(ytest_path)\n","\n","\n","# List of numerical features in the dataset\n","numeric_features = [\n","    'CreditScore',       # Customer's credit score\n","    'Age',               # Customer's age\n","    'Tenure',            # Number of years the customer has been with the bank\n","    'Balance',           # Customer’s account balance\n","    'NumOfProducts',     # Number of products the customer has with the bank\n","    'HasCrCard',         # Whether the customer has a credit card (binary: 0 or 1)\n","    'IsActiveMember',    # Whether the customer is an active member (binary: 0 or 1)\n","    'EstimatedSalary'    # Customer’s estimated salary\n","]\n","\n","# List of categorical features in the dataset\n","categorical_features = [\n","    'Geography',         # Country where the customer resides\n","]\n","\n","\n","# Set the clas weight to handle class imbalance\n","class_weight = ytrain.value_counts()[0] / ytrain.value_counts()[1]\n","class_weight\n","\n","# Define the preprocessing steps\n","preprocessor = make_column_transformer(\n","    (StandardScaler(), numeric_features),\n","    (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",")\n","\n","# Define base XGBoost model\n","xgb_model = xgb.XGBClassifier(scale_pos_weight=class_weight, random_state=42)\n","\n","# Define hyperparameter grid\n","param_grid = {\n","    'xgbclassifier__n_estimators': [50, 75, 100, 125, 150],    # number of tree to build\n","    'xgbclassifier__max_depth': [2, 3, 4],    # maximum depth of each tree\n","    'xgbclassifier__colsample_bytree': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each tree\n","    'xgbclassifier__colsample_bylevel': [0.4, 0.5, 0.6],    # percentage of attributes to be considered (randomly) for each level of a tree\n","    'xgbclassifier__learning_rate': [0.01, 0.05, 0.1],    # learning rate\n","    'xgbclassifier__reg_lambda': [0.4, 0.5, 0.6],    # L2 regularization factor\n","}\n","\n","# Model pipeline\n","model_pipeline = make_pipeline(preprocessor, xgb_model)\n","\n","# Hyperparameter tuning with GridSearchCV\n","grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, n_jobs=-1)\n","grid_search.fit(Xtrain, ytrain)\n","\n","\n","# Check the parameters of the best model\n","grid_search.best_params_\n","\n","# Store the best model\n","best_model = grid_search.best_estimator_\n","best_model\n","\n","# Set the classification threshold\n","classification_threshold = 0.45\n","\n","# Make predictions on the training data\n","y_pred_train_proba = best_model.predict_proba(Xtrain)[:, 1]\n","y_pred_train = (y_pred_train_proba >= classification_threshold).astype(int)\n","\n","# Make predictions on the test data\n","y_pred_test_proba = best_model.predict_proba(Xtest)[:, 1]\n","y_pred_test = (y_pred_test_proba >= classification_threshold).astype(int)\n","\n","# Generate a classification report to evaluate model performance on training set\n","print(classification_report(ytrain, y_pred_train))\n","\n","# Generate a classification report to evaluate model performance on test set\n","print(classification_report(ytest, y_pred_test))\n","\n","# Save best model\n","joblib.dump(best_model, \"best_churn_model.joblib\")\n","\n","# Upload to Hugging Face\n","repo_id = \"<-------Hugging Face User ID-------->/churn-model\"\n","repo_type = \"model\"\n","\n","api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n","\n","# Step 1: Check if the space exists\n","try:\n","    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n","    print(f\"Model Space '{repo_id}' already exists. Using it.\")\n","except RepositoryNotFoundError:\n","    print(f\"Model Space '{repo_id}' not found. Creating new space...\")\n","    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n","    print(f\"Model Space '{repo_id}' created.\")\n","\n","# create_repo(\"churn-model\", repo_type=\"model\", private=False)\n","api.upload_file(\n","    path_or_fileobj=\"best_churn_model.joblib\",\n","    path_in_repo=\"best_churn_model.joblib\",\n","    repo_id=repo_id,\n","    repo_type=repo_type,\n",")"],"metadata":{"id":"MrmLP6OkBbqm","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"error","timestamp":1769707155806,"user_tz":-330,"elapsed":220,"user":{"displayName":"Manju krishna","userId":"17824596203469046751"}},"outputId":"6ba57d48-712a-45f5-afc0-14306f0a12c3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing mlops/model_building/train.py\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'mlops/model_building/train.py'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1590725901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mlops/model_building/train.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# for data manipulation\\nimport pandas as pd\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.pipeline import make_pipeline\\n# for model training, tuning, and evaluation\\nimport xgboost as xgb\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import accuracy_score, classification_report, recall_score\\n# for model serialization\\nimport joblib\\n# for creating a folder\\nimport os\\n# for hugging face space authentication to upload files\\nfrom huggingface_hub import login, HfApi, create_repo\\nfrom huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\\n\\napi = HfApi()\\n\\nXtrain_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/Xtrain.csv\"\\nXtest_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/Xtest.csv\"\\nytrain_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/ytrain.csv\"\\nytest_path = \"hf://datasets/<-------Hugging Face User ID-------->/bank-customer-churn/ytest.csv\"\\n\\nXtrain = pd.read_csv(Xtrain_path)\\nXtest = pd.read_csv(Xtest_path)\\nytrain = pd.read_csv(ytrain_path)\\nytest = pd.read_csv(ytest_path)\\n\\n\\n# List of numerical features in the dataset\\nnumeric_featu...\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-98>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlops/model_building/train.py'"]}]},{"cell_type":"markdown","source":["# Deployment"],"metadata":{"id":"0McYCZzkji5I"}},{"cell_type":"markdown","source":["## Dockerfile"],"metadata":{"id":"9QrY2v77vbEZ"}},{"cell_type":"code","source":["os.makedirs(\"mlops/deployment\", exist_ok=True)"],"metadata":{"id":"0-AMAI72CR-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile mlops/deployment/Dockerfile\n","# Use a minimal base image with Python 3.9 installed\n","FROM python:3.9\n","\n","# Set the working directory inside the container to /app\n","WORKDIR /app\n","\n","# Copy all files from the current directory on the host to the container's /app directory\n","COPY . .\n","\n","# Install Python dependencies listed in requirements.txt\n","RUN pip3 install -r requirements.txt\n","\n","RUN useradd -m -u 1000 user\n","USER user\n","ENV HOME=/home/user \\\n","\tPATH=/home/user/.local/bin:$PATH\n","\n","WORKDIR $HOME/app\n","\n","COPY --chown=user . $HOME/app\n","\n","# Define the command to run the Streamlit app on port \"8501\" and make it accessible externally\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"],"metadata":{"id":"ZTicTDnPCVZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Streamlit App"],"metadata":{"id":"LCvrklrBwNvJ"}},{"cell_type":"code","source":["%%writefile mlops/deployment/app.py\n","import streamlit as st\n","import pandas as pd\n","from huggingface_hub import hf_hub_download\n","import joblib\n","\n","# Download the model from the Model Hub\n","model_path = hf_hub_download(repo_id=\"<-------Hugging Face User ID-------->/churn-model\", filename=\"best_churn_model_v1.joblib\")\n","\n","# Load the model\n","model = joblib.load(model_path)\n","\n","# Streamlit UI for Customer Churn Prediction\n","st.title(\"Customer Churn Prediction App\")\n","st.write(\"The Customer Churn Prediction App is an internal tool for bank staff that predicts whether customers are at risk of churning based on their details.\")\n","st.write(\"Kindly enter the customer details to check whether they are likely to churn.\")\n","\n","# Collect user input\n","CreditScore = st.number_input(\"Credit Score (customer's credit score)\", min_value=300, max_value=900, value=650)\n","Geography = st.selectbox(\"Geography (country where the customer resides)\", [\"France\", \"Germany\", \"Spain\"])\n","Age = st.number_input(\"Age (customer's age in years)\", min_value=18, max_value=100, value=30)\n","Tenure = st.number_input(\"Tenure (number of years the customer has been with the bank)\", value=12)\n","Balance = st.number_input(\"Account Balance (customer’s account balance)\", min_value=0.0, value=10000.0)\n","NumOfProducts = st.number_input(\"Number of Products (number of products the customer has with the bank)\", min_value=1, value=1)\n","HasCrCard = st.selectbox(\"Has Credit Card?\", [\"Yes\", \"No\"])\n","IsActiveMember = st.selectbox(\"Is Active Member?\", [\"Yes\", \"No\"])\n","EstimatedSalary = st.number_input(\"Estimated Salary (customer’s estimated salary)\", min_value=0.0, value=50000.0)\n","\n","# Convert categorical inputs to match model training\n","input_data = pd.DataFrame([{\n","    'CreditScore': CreditScore,\n","    'Geography': Geography,\n","    'Age': Age,\n","    'Tenure': Tenure,\n","    'Balance': Balance,\n","    'NumOfProducts': NumOfProducts,\n","    'HasCrCard': 1 if HasCrCard == \"Yes\" else 0,\n","    'IsActiveMember': 1 if IsActiveMember == \"Yes\" else 0,\n","    'EstimatedSalary': EstimatedSalary\n","}])\n","\n","# Set the classification threshold\n","classification_threshold = 0.45\n","\n","# Predict button\n","if st.button(\"Predict\"):\n","    prediction_proba = model.predict_proba(input_data)[0, 1]\n","    prediction = (prediction_proba >= classification_threshold).astype(int)\n","    result = \"churn\" if prediction == 1 else \"not churn\"\n","    st.write(f\"Based on the information provided, the customer is likely to {result}.\")"],"metadata":{"id":"jzge_a2LCdWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dependency Handling"],"metadata":{"id":"07cYzWcIwTL-"}},{"cell_type":"code","source":["%%writefile mlops/deployment/requirements.txt\n","pandas==2.2.2\n","huggingface_hub==0.32.6\n","streamlit==1.43.2\n","joblib==1.5.1\n","scikit-learn==1.6.0\n","xgboost==2.1.4"],"metadata":{"id":"NgW5sT83ChI2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hosting"],"metadata":{"id":"V4ynzpKNwWS_"}},{"cell_type":"code","source":["os.makedirs(\"mlops/hosting\", exist_ok=True)"],"metadata":{"id":"Blkwc3ggByKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile mlops/hosting/hosting.py\n","from huggingface_hub import HfApi\n","import os\n","\n","api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n","api.upload_folder(\n","    folder_path=\"mlops/deployment\",     # the local folder containing your files\n","    repo_id=\"<-------Hugging Face User ID-------->/Bank-Customer-Churn\",          # the target repo\n","    repo_type=\"space\",                      # dataset, model, or space\n","    path_in_repo=\"\",                          # optional: subfolder path inside the repo\n",")"],"metadata":{"id":"fV6vwNqiBwrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create MLOps pipeline with Github Action Workflow"],"metadata":{"id":"PuCgAW2hktli"}},{"cell_type":"markdown","source":["## Action Workflow YAML File"],"metadata":{"id":"yo_xWLLvkqSC"}},{"cell_type":"markdown","source":["* A YAML file is a simple, human-readable file used to store configuration settings.\n","* YAML stands for Yet Another Markup Language or YAML Ain't Markup Language (a recursive acronym).\n","* It uses indentation (spaces) to show structure, like folders inside folders.\n","* Each line contains a key and a value, making it easy to organize data.\n","* YAML is often used in automation tools, cloud setups, and app settings."],"metadata":{"id":"ICEDG3zwkLh2"}},{"cell_type":"markdown","source":["Here's the YAML file we'd need for our use case."],"metadata":{"id":"C5HihLKDnJfG"}},{"cell_type":"markdown","source":["```\n","name: MLOps pipeline\n","\n","on:\n","  workflow_dispatch:\n","\n","jobs:\n","\n","  register-dataset:\n","    runs-on: ubuntu-latest\n","    steps:\n","      - uses: actions/checkout@v3\n","      - name: Install Dependencies\n","        run: pip install -r mlops/requirements.txt\n","      - name: Upload Dataset to Hugging Face Hub\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: python mlops/model_building/data_register.py\n","\n","  data-prep:\n","    needs: register-dataset\n","    runs-on: ubuntu-latest\n","    steps:\n","      - uses: actions/checkout@v3\n","      - name: Install Dependencies\n","        run: pip install -r mlops/requirements.txt\n","      - name: Run Data Preparation\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: python mlops/model_building/prep.py\n","\n","\n","  model-traning:\n","    needs: data-prep\n","    runs-on: ubuntu-latest\n","    steps:\n","      - uses: actions/checkout@v3\n","      - name: Install Dependencies\n","        run: pip install -r mlops/requirements.txt\n","      - name: Model Building\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: python mlops/model_building/train.py\n","\n","\n","  deploy-hosting:\n","    runs-on: ubuntu-latest\n","    needs: [model-traning,data-prep,register-dataset]\n","    steps:\n","      - uses: actions/checkout@v3\n","      - name: Install Dependencies\n","        run: pip install -r mlops/requirements.txt\n","      - name: Push files to Frontend Hugging Face Space\n","        env:\n","          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n","        run: python mlops/hosting/hosting.py\n","\n","```"],"metadata":{"id":"J029tYPq4Rmq"}},{"cell_type":"markdown","source":["**Note:** To use this YAML file for our use case, we need to\n","\n","1. Go to the GitHub repository for the project\n","2. Create a folder named ***.github/workflows/***\n","3. In the above folder, create a file named ***pipeline.yml***\n","4. Copy and paste the above content for the YAML file into the ***pipeline.yml*** file"],"metadata":{"id":"T9fgZ_Mq3zzp"}},{"cell_type":"markdown","source":["## Requirements file for the Github Action Workflow"],"metadata":{"id":"PvEUJ-t5kdxH"}},{"cell_type":"code","source":["%%writefile mlops/requirements.txt\n","huggingface_hub==0.32.6\n","datasets==3.6.0\n","pandas==2.2.2\n","scikit-learn==1.6.0\n","xgboost==2.1.4"],"metadata":{"id":"tePQBrGOCn2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Github Authentication and Push Files"],"metadata":{"id":"BA6mP-Ebkm3O"}},{"cell_type":"markdown","source":["* Before moving forward, we need to generate a secret token to push files directly from Colab to the GitHub repository.\n","* Please follow the below instructions to create the GitHub token:\n","    - Open your GitHub profile.\n","    - Click on ***Settings***.\n","    - Go to ***Developer Settings***.\n","    - Expand the ***Personal access tokens*** section and select ***Tokens (classic)***.\n","    - Click ***Generate new token***, then choose ***Generate new token (classic)***.\n","    - Add a note and select all required scopes.\n","    - Click ***Generate token***.\n","    - Copy the generated token and store it safely in a notepad."],"metadata":{"id":"T84Ei-g9Z2uw"}},{"cell_type":"code","source":["# Install Git\n","!apt-get install git\n","\n","# Set your Git identity (replace with your details)\n","!git config --global user.email \"<--------Github Email ID----------->\"\n","!git config --global user.name \"<---------Github User Name---------->\"\n","\n","# Clone your GitHub repository\n","!git clone https://github.com/<---------Github User Name---------->/<------Github Repo Name------->.git\n","\n","# Move your folder to the repository directory\n","!mv /content/mlops/ /content/<------Github Repo Name------->"],"metadata":{"id":"KPDx4gqGh7cO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770120106770,"user_tz":-330,"elapsed":12183,"user":{"displayName":"Manju krishna","userId":"17824596203469046751"}},"outputId":"a342c06d-4f6a-4ebe-a3b0-99df473d4f14"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.15).\n","0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n","/bin/bash: line 1: ---------Github: No such file or directory\n","/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n","/bin/bash: -c: line 1: `mv /content/mlops/ /content/<------Github Repo Name------->'\n"]}]},{"cell_type":"code","source":["# Change directory to the cloned repository\n","%cd <------Github Repo Name------->/\n","\n","# Add the new folder to Git\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"first commit\"\n","\n","# Push to GitHub (you'll need your GitHub credentials; use a personal access token if 2FA enabled)\n","!git push https://<------Github User Name------->:<------Github Token------->@github.com/<------Github User Name------->/<------Github Repo Name------->.git"],"metadata":{"id":"IuUahCwVigon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font size=6 color=\"navyblue\">Power Ahead!</font>\n","___"],"metadata":{"id":"fN8j9-3nW8G9"}}]}